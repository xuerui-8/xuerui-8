import requests
import concurrent.futures
import sys
from urllib.parse import urljoin

def check_url(url):
    if not url.startswith(('http://', 'https://')):
        url = 'http://' + url

    target_url = urljoin(url, 'sobey-mchEditor/watermark/upload')

    try:
        response = requests.get(target_url, timeout=5)

        if response.status_code == 200:
            return f"{url} - Status Code: 200"
        return None
    except requests.RequestException:
        return None

def main():
    urls = []
    if len(sys.argv) > 1:
        try:
            with open(sys.argv[1], 'r') as file:
                urls = [line.strip() for line in file if line.strip()]
        except FileNotFoundError:
            print(f"Error: File {sys.argv[1]} not found")
            sys.exit(1)
    else:
        # Read URLs from stdin
        print("Enter URLs (one per line, press Ctrl+D or Ctrl+Z when done):")
        urls = [line.strip() for line in sys.stdin if line.strip()]

    if not urls:
        print("No URLs provided")
        sys.exit(1)

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(check_url, urls)

        found = False
        for result in results:
            if result:
                print(result)
                found = True

        if not found:
            print("No websites found")

if __name__ == "__main__":
    main()
